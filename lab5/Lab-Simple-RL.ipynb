{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d871bf",
   "metadata": {},
   "source": [
    "**Objective:** \n",
    "Your task is to program an agent to find the optimal policy for navigating a labyrinth from a specified starting point to a goal point using the Value Iteration algorithm.\n",
    "\n",
    "**Step 1: Familiarize with the Environment**\n",
    "- Understand the structure of the `Labyrinth` class and how it represents the labyrinth environment, including walls, the starting point, and the goal.\n",
    "- Familiarize yourself with how the `Agent` class is structured, and how it interacts with the labyrinth environment.\n",
    "\n",
    "**Step 2: Implement Value Iteration**\n",
    "- Create a function or method to implement the Value Iteration algorithm.\n",
    "- You'll need to initialize a utility table with zeros and iteratively update the utilities of each state (i.e., each cell in the labyrinth) based on the Bellman equation.\n",
    "- The stopping criterion for Value Iteration is when the maximum change in utility is less than a small threshold, say 0.01.\n",
    "- Once the utilities have converged, use them to compute the optimal policy, which specifies the best action to take in each state.\n",
    "\n",
    "**Step 3: Modify the Agent Class**\n",
    "- Modify the `act` method of the `Agent` class to use the optimal policy derived from Value Iteration instead of taking random actions.\n",
    "- Optionally, you can also modify the `update` method to incorporate any additional learning or updating you wish to implement.\n",
    "\n",
    "**Step 4: Run the Simulation**\n",
    "- Run the provided simulation loop, where the agent is placed in the labyrinth and must navigate to the goal.\n",
    "- Observe how the agent's behavior changes as it learns the optimal policy.\n",
    "- You might want to add some print statements or other logging to help visualize the agent's path through the labyrinth and how it improves over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee24ce55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T18:33:15.766043Z",
     "start_time": "2024-10-07T18:33:15.758450Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from time import sleep\n",
    "\n",
    "class Labyrinth:\n",
    "    def __init__(self, rows, cols, walls, start, goal):\n",
    "        self.walls = walls\n",
    "        self.grid = np.zeros((rows, cols))\n",
    "        for wall in walls:\n",
    "            self.grid[wall] = -1  # Assign -1 for walls\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.current_position = start\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = self.start\n",
    "        return self.current_position\n",
    "\n",
    "    def render(self, epoch=0, t=0, sleep_time=1):\n",
    "        print(f'epoch: {epoch}, t: {t}')\n",
    "        grid_copy = self.grid.copy()\n",
    "        grid_copy[self.current_position] = 2\n",
    "        grid_copy[self.goal] = 9\n",
    "        display(grid_copy)\n",
    "        print('-' * 50)\n",
    "        sleep(sleep_time)\n",
    "\n",
    "    def step(self, action: tuple[int, int]):\n",
    "        # Assume actions are encoded as (delta_row, delta_col)\n",
    "        new_position = (self.current_position[0] + action[0], self.current_position[1] + action[1])\n",
    "        if self.is_valid_move(new_position):\n",
    "            self.current_position = new_position\n",
    "        reward = 1 if self.current_position == self.goal else 0\n",
    "        return self.current_position, reward\n",
    "\n",
    "    def is_valid_move(self, position):\n",
    "        rows, cols = self.grid.shape\n",
    "        return 0 <= position[0] < rows and 0 <= position[1] < cols and self.grid[position] != -1\n",
    "\n",
    "    def done(self):\n",
    "        return self.current_position == self.goal\n",
    "\n",
    "\n",
    "class Action:\n",
    "    def __init__(self, action, state):\n",
    "        self.action = action\n",
    "        self.state = state\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, lab: Labyrinth, gamma=0.5):\n",
    "        self.lab = lab\n",
    "        self.utility_table = np.zeros(lab.grid.shape)\n",
    "\n",
    "        for wall in lab.walls:\n",
    "            self.utility_table[wall] = -1\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.actions = {\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1),\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0)\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        next_best_state = self.select_best_action(state).action\n",
    "        best_action = self.actions[next_best_state]\n",
    "        # return random.choice([(0, 1), (0, -1), (1, 0), (-1, 0)])  # Random action for demonstration\n",
    "        return best_action\n",
    "\n",
    "    def max_a_utility(self, state: tuple[int, int]):\n",
    "        return max([self.utility_table[s.state] for s in self.potential_states(state)])\n",
    "\n",
    "    def select_best_action(self, state) -> Action:\n",
    "        return max(self.potential_states(state), key=lambda a: self.utility_table[a.state])\n",
    "\n",
    "    def potential_states(self, state):\n",
    "        potential = []\n",
    "        rows, cols = self.utility_table.shape\n",
    "\n",
    "        for a in self.actions:\n",
    "            s_new_y, s_new_x = self.actions[a]\n",
    "            s_new_y += state[0]\n",
    "            s_new_x += state[1]\n",
    "\n",
    "            if (0 <= s_new_y < rows and 0 <= s_new_x < cols) and (s_new_y, s_new_x) not in self.lab.walls:\n",
    "                potential.append(\n",
    "                    Action(a, (s_new_y, s_new_x))\n",
    "                )\n",
    "\n",
    "        return potential\n",
    "\n",
    "    def update_utility_table(self):\n",
    "        new_utility_table = np.copy(self.utility_table)\n",
    "\n",
    "        for y in range(self.utility_table.shape[0]):\n",
    "            for x in range(self.utility_table.shape[1]):\n",
    "                s = (y, x)\n",
    "                if s not in self.lab.walls:\n",
    "                    new_utility_table[s] = self.bellman_equation(s)\n",
    "\n",
    "        self.utility_table = new_utility_table\n",
    "\n",
    "    def bellman_equation(self, s):\n",
    "        return self.get_reward(s) + self.gamma * self.max_a_utility(s)\n",
    "\n",
    "    def update(self, action, state, reward):\n",
    "        pass\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        if state == self.lab.goal:\n",
    "            return 1\n",
    "        elif state in self.lab.walls:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T18:33:22.186656Z",
     "start_time": "2024-10-07T18:33:22.023620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define labyrinth\n",
    "labyrinth = Labyrinth(4, 4, {(1, 1), (2, 1), (1, 2)}, (0, 0), (3, 3))\n",
    "agent = Agent(lab=labyrinth, gamma=0.5)\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "T = 100\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state = labyrinth.reset()\n",
    "    agent.reset()\n",
    "\n",
    "    for t in range(T):\n",
    "        agent.update_utility_table()\n",
    "        action = agent.act(state)\n",
    "        state, reward = labyrinth.step(action)\n",
    "        if labyrinth.done():\n",
    "            break\n",
    "agent.utility_table"
   ],
   "id": "587658c8d03726",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02083333,  0.04166667,  0.08333333,  0.16666667],\n",
       "       [ 0.04166667, -1.        , -1.        ,  0.33333333],\n",
       "       [ 0.08333333, -1.        ,  0.33333333,  0.66666667],\n",
       "       [ 0.16666667,  0.33333333,  0.66666667,  1.33333333]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T18:33:28.123255Z",
     "start_time": "2024-10-07T18:33:24.109436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test agent\n",
    "state = labyrinth.reset()\n",
    "agent.reset()\n",
    "\n",
    "print(\"let's get it! 2 is agent, 9 is goal\")\n",
    "labyrinth.render()\n",
    "\n",
    "for t in range(T):\n",
    "    action = agent.act(state)\n",
    "    state, reward = labyrinth.step(action)\n",
    "    \n",
    "    labyrinth.render(epoch=-1, t=t, sleep_time=0.5)\n",
    "    if labyrinth.done():\n",
    "        print(\"Goal reached!\\n\")\n",
    "        break"
   ],
   "id": "9af2aee34a131db8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's get it! 2 is agent, 9 is goal\n",
      "epoch: 0, t: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  0.,  0.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch: -1, t: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  0.,  0.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch: -1, t: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  2.,  0.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch: -1, t: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  2.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch: -1, t: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0., -1., -1.,  2.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch: -1, t: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  2.],\n",
       "       [ 0.,  0.,  0.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "epoch: -1, t: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Goal reached!\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "350214c6c961b48",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
