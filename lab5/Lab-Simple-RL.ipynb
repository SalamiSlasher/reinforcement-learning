{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d871bf",
   "metadata": {},
   "source": [
    "**Objective:** \n",
    "Your task is to program an agent to find the optimal policy for navigating a labyrinth from a specified starting point to a goal point using the Value Iteration algorithm.\n",
    "\n",
    "**Step 1: Familiarize with the Environment**\n",
    "- Understand the structure of the `Labyrinth` class and how it represents the labyrinth environment, including walls, the starting point, and the goal.\n",
    "- Familiarize yourself with how the `Agent` class is structured, and how it interacts with the labyrinth environment.\n",
    "\n",
    "**Step 2: Implement Value Iteration**\n",
    "- Create a function or method to implement the Value Iteration algorithm.\n",
    "- You'll need to initialize a utility table with zeros and iteratively update the utilities of each state (i.e., each cell in the labyrinth) based on the Bellman equation.\n",
    "- The stopping criterion for Value Iteration is when the maximum change in utility is less than a small threshold, say 0.01.\n",
    "- Once the utilities have converged, use them to compute the optimal policy, which specifies the best action to take in each state.\n",
    "\n",
    "**Step 3: Modify the Agent Class**\n",
    "- Modify the `act` method of the `Agent` class to use the optimal policy derived from Value Iteration instead of taking random actions.\n",
    "- Optionally, you can also modify the `update` method to incorporate any additional learning or updating you wish to implement.\n",
    "\n",
    "**Step 4: Run the Simulation**\n",
    "- Run the provided simulation loop, where the agent is placed in the labyrinth and must navigate to the goal.\n",
    "- Observe how the agent's behavior changes as it learns the optimal policy.\n",
    "- You might want to add some print statements or other logging to help visualize the agent's path through the labyrinth and how it improves over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee24ce55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T16:17:38.592186Z",
     "start_time": "2024-10-07T16:17:38.585111Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from time import sleep\n",
    "\n",
    "class Labyrinth:\n",
    "    def __init__(self, rows, cols, walls, start, goal):\n",
    "        self.walls = walls\n",
    "        self.grid = np.zeros((rows, cols))\n",
    "        for wall in walls:\n",
    "            self.grid[wall] = -1  # Assign -1 for walls\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.current_position = start\n",
    "        \n",
    "        self.render()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_position = self.start\n",
    "        return self.current_position\n",
    "    \n",
    "    def render(self, epoch=0, t=0, sleep_time=1):\n",
    "        print(f'epoch: {epoch}, t: {t}')\n",
    "        grid_copy = self.grid.copy()\n",
    "        grid_copy[self.current_position] = 2\n",
    "        grid_copy[self.goal] = 9\n",
    "        display(grid_copy)\n",
    "        print('-' * 50)\n",
    "        sleep(sleep_time)\n",
    "        \n",
    "    \n",
    "    def step(self, action: tuple[int, str]):\n",
    "        # Assume actions are encoded as (delta_row, delta_col)\n",
    "        new_position = (self.current_position[0] + action[0], self.current_position[1] + action[1])\n",
    "        if self.is_valid_move(new_position):\n",
    "            self.current_position = new_position\n",
    "        reward = 1 if self.current_position == self.goal else 0\n",
    "        return self.current_position, reward\n",
    "\n",
    "    def is_valid_move(self, position):\n",
    "        rows, cols = self.grid.shape\n",
    "        return 0 <= position[0] < rows and 0 <= position[1] < cols and self.grid[position] != -1\n",
    "\n",
    "    def done(self):\n",
    "        return self.current_position == self.goal\n",
    "\n",
    "class Action:\n",
    "    def __init__(self, action, state):\n",
    "        self.action = action\n",
    "        self.state = state\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, walls, gamma = 0.5):\n",
    "        \n",
    "        self.utility_table = np.zeros(labyrinth.grid.shape)\n",
    "        for wall in walls:\n",
    "            self.utility_table[wall] = -1\n",
    "        print('utility table:')\n",
    "        display(self.utility_table)\n",
    "        print('-' * 50)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.actions = {\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1),\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0)\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        pass  # Reset any agent state here\n",
    "    \n",
    "    def act(self, state):\n",
    "        next_best_state = self.select_best_action(state).action\n",
    "        \n",
    "        #return random.choice([(0, 1), (0, -1), (1, 0), (-1, 0)])  # Random action for demonstration\n",
    "        return self.actions[next_best_state]\n",
    "    \n",
    "    def max_a_reward(self, state: tuple[int, int]):\n",
    "        return max([self.utility_table[s.state] for s in self.potential_states(state)])\n",
    "    \n",
    "    def select_best_action(self, state) -> Action:\n",
    "        #print('gg')\n",
    "        return max(self.potential_states(state), key=lambda a: self.utility_table[a.state])\n",
    "        # return max(self.potential_states(state), key=self.utility_table[state])\n",
    "    \n",
    "    def potential_states(self, state):\n",
    "        potential = []\n",
    "        rows, cols = self.utility_table.shape\n",
    "        \n",
    "        for a in self.actions:\n",
    "            s_new_y, s_new_x = self.actions[a]\n",
    "            s_new_y += state[0]\n",
    "            s_new_x += state[1]\n",
    "            \n",
    "            if 0 <= s_new_y < rows and 0 <= s_new_x < cols:\n",
    "                potential.append(\n",
    "                    Action(a, (s_new_y, s_new_x))\n",
    "                )\n",
    "            \n",
    "        return potential\n",
    "        \n",
    "    def update(self, action, state, reward):\n",
    "        self.utility_table[action] = reward + self.gamma * self.max_a_reward(state)\n",
    "        \n",
    "        pass  # Update any agent state here"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T16:18:32.309449Z",
     "start_time": "2024-10-07T16:18:30.953810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define labyrinth\n",
    "labyrinth = Labyrinth(4, 4, {(1, 1), (2, 1), (1, 2)}, (0, 0), (3, 3))\n",
    "agent = Agent(walls=labyrinth.walls)\n",
    "\n",
    "MAX_EPISODES = 1000\n",
    "T = 100\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    state = labyrinth.reset()\n",
    "    #print(state)\n",
    "    agent.reset()\n",
    "    \n",
    "    for t in range(T):\n",
    "        action = agent.act(state)\n",
    "        state, reward = labyrinth.step(action)\n",
    "        agent.update(action, state, reward)\n",
    "        #labyrinth.render(epoch=episode, t=t)\n",
    "        if labyrinth.done():\n",
    "            print(\"ez win\")\n",
    "            break\n",
    "    \n",
    "    #labyrinth.render(epoch=episode)"
   ],
   "id": "587658c8d03726",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, t: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  0.,  0.,  0.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  9.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "utility table:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "5b8a43be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T16:18:22.891063Z",
     "start_time": "2024-10-07T16:18:22.886594Z"
    }
   },
   "source": "agent.utility_table",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.],\n",
       "       [ 0., -1., -1.,  0.],\n",
       "       [ 0., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "844e2329ef6882b4",
   "metadata": {},
   "source": "labyrinth.grid",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e6ebdedfc8dc350f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b875fe0e987ec272",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
